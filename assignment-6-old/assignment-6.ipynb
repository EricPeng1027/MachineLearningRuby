{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Model Evaluation and Regularization\n",
    "\n",
    "\n",
    "In this assignment, we will investigate two evaluation methods and two ways that regularization can be used to control the behavior of linear models. Most of the code here will be copied or refactored from previous assignments. You are encouraged to copy **your code only** from previous assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if(window['d3'] === undefined ||\n",
       "   window['Nyaplot'] === undefined){\n",
       "    var path = {\"d3\":\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\",\"downloadable\":\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\"};\n",
       "\n",
       "\n",
       "\n",
       "    var shim = {\"d3\":{\"exports\":\"d3\"},\"downloadable\":{\"exports\":\"downloadable\"}};\n",
       "\n",
       "    require.config({paths: path, shim:shim});\n",
       "\n",
       "\n",
       "require(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\n",
       "\n",
       "\tvar script = d3.select(\"head\")\n",
       "\t    .append(\"script\")\n",
       "\t    .attr(\"src\", \"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\")\n",
       "\t    .attr(\"async\", true);\n",
       "\n",
       "\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\n",
       "\n",
       "\n",
       "\t    var event = document.createEvent(\"HTMLEvents\");\n",
       "\t    event.initEvent(\"load_nyaplot\",false,false);\n",
       "\t    window.dispatchEvent(event);\n",
       "\t    console.log('Finished loading Nyaplotjs');\n",
       "\n",
       "\t};\n",
       "\n",
       "\n",
       "});});\n",
       "}\n"
      ],
      "text/plain": [
       "\"if(window['d3'] === undefined ||\\n   window['Nyaplot'] === undefined){\\n    var path = {\\\"d3\\\":\\\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\\\",\\\"downloadable\\\":\\\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\\\"};\\n\\n\\n\\n    var shim = {\\\"d3\\\":{\\\"exports\\\":\\\"d3\\\"},\\\"downloadable\\\":{\\\"exports\\\":\\\"downloadable\\\"}};\\n\\n    require.config({paths: path, shim:shim});\\n\\n\\nrequire(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\\n\\n\\tvar script = d3.select(\\\"head\\\")\\n\\t    .append(\\\"script\\\")\\n\\t    .attr(\\\"src\\\", \\\"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\\\")\\n\\t    .attr(\\\"async\\\", true);\\n\\n\\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\\n\\n\\n\\t    var event = document.createEvent(\\\"HTMLEvents\\\");\\n\\t    event.initEvent(\\\"load_nyaplot\\\",false,false);\\n\\t    window.dispatchEvent(event);\\n\\t    console.log('Finished loading Nyaplotjs');\\n\\n\\t};\\n\\n\\n});});\\n}\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require './assignment_lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1.1 (1 Point)\n",
    "\n",
    "Copy **YOUR** implementation of ```StochasticGradientDescent``` from [Assignment 5](../assignment-5/assignment-5.ipynb) into the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9d2c75967facbcccd8bf764b464cdaea",
     "grade": false,
     "grade_id": "cell-302b13014a2e7383",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":update"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN YOUR CODE\n",
    "class StochasticGradientDescent\n",
    "  attr_reader :weights\n",
    "  attr_reader :objective\n",
    "  def initialize obj, w_0, lr = 0.01\n",
    "    @objective = obj\n",
    "    @weights = w_0\n",
    "    @n = 1.0\n",
    "    @lr = lr\n",
    "  end\n",
    "  def update x\n",
    "    # BEGIN YOUR CODE\n",
    "    g = @objective.grad(x, @weights)\n",
    "    learning_rate = @lr / Math.sqrt(@n)\n",
    "    @weights.each do |k, v|\n",
    "      @weights[k] -= g[k] * learning_rate\n",
    "    end\n",
    "    @n += 1.0\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end\n",
    "#END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07f27ff123a189dc5d56c0daf17b45f4",
     "grade": true,
     "grade_id": "cell-6b98a23865128aa3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Hidden Test (See Test 1.1 from Assignment 5) ###\n",
    "assert_not_nil(StochasticGradientDescent.class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2 (1 point)\n",
    "\n",
    "Copy **YOUR** implementation of the ```dot``` product and ```norm``` functions from [Assignment 4](../assignment-4/assignment-4.ipynb) into the following cell. Please copy the whole function, not just the parts within the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "30028c7c84bfe7cbbf5a285c0034c120",
     "grade": false,
     "grade_id": "cell-92f3de7f5b9a8b90",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":norm"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN YOUR CODE\n",
    "#Implement the error function given a weight vector, w\n",
    "def dot x, w\n",
    "  # BEGIN YOUR CODE\n",
    "  sum = 0.0\n",
    "    \n",
    "    if !(x.empty? or w.empty?)\n",
    "      x.each do |k, v|\n",
    "          if w.has_key?(k)\n",
    "              sum += v * w[k]\n",
    "          end\n",
    "      end\n",
    "    end\n",
    "    \n",
    "    return sum\n",
    "  #END YOUR CODE\n",
    "end\n",
    "\n",
    "def norm w\n",
    "  # BEGIN YOUR CODE\n",
    "  return Math.sqrt(dot(w, w))\n",
    "  #END YOUR CODE\n",
    "end\n",
    "#END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dd9e10acbe77b96edf646b4e59c6c71e",
     "grade": true,
     "grade_id": "cell-c0a5c2a265dea170",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_in_delta 2.0, norm({\"a\" => 1.41421, \"b\" => 1.41421}), 1e-2\n",
    "assert_in_delta 2.0, norm({\"a\" => -1.41421, \"b\" => 1.41421}), 1e-2\n",
    "assert_in_delta 0.0, norm({}), 1e-2\n",
    "\n",
    "assert_in_delta 6.0, dot({\"a\" => 2.0}, {\"a\" => 3.0}), 1e-6\n",
    "assert_in_delta 6.0, dot({\"a\" => 2.0}, {\"a\" => 3.0, \"b\" => 4.0}), 1e-6\n",
    "assert_equal 0.0, dot({}, {})\n",
    "assert_equal 0.0, dot({\"a\" => 1.0}, {\"b\" => 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3 (1 Point)\n",
    "\n",
    "Refactor **YOUR** $z$-score normalization method from [Assignment 5](../assignment-5/assignment-5.ipynb), where we called it ```create_zspambase```. It should be general enough to normalize any dataset. Only normalize features in the ```features``` key.\n",
    "\n",
    "Note: Watch out for zero-stdev features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ab794f199cdb7647057279d6879187ef",
     "grade": false,
     "grade_id": "cell-7c0c300e6a7b2de7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":z_normalize"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def z_normalize dataset\n",
    "  zdataset = dataset.clone\n",
    "  zdataset[\"data\"] = dataset[\"data\"].collect do |r|\n",
    "    u = r.clone\n",
    "    u[\"features\"] = r[\"features\"].clone\n",
    "    u\n",
    "  end\n",
    "\n",
    "  # BEGIN YOUR CODE\n",
    "  mu = Hash.new\n",
    "  dev = Hash.new\n",
    "  data = zdataset[\"data\"]\n",
    "  \n",
    "  zdataset[\"features\"].each do |fname|\n",
    "    processed_data = data.collect {|row| row[\"features\"][fname]}\n",
    "    processed_data = processed_data.select {|x| x != nil }\n",
    "    mu[fname] = mean(processed_data)\n",
    "    dev[fname] = stdev(processed_data)\n",
    "  end\n",
    "  \n",
    "  zdataset[\"features\"].each do |fname|\n",
    "    data.each do |row|\n",
    "      #set missing values to 0.0\n",
    "      row[\"features\"][fname] = dev[fname] == 0.0 ? 0.0 : (row[\"features\"][fname] == nil ? 0.0 : (row[\"features\"][fname] - mu[fname]) / dev[fname])\n",
    "    end\n",
    "  end\n",
    "  #END YOUR CODE\n",
    "  return zdataset\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "20296cc052a36277656f0780d762677b",
     "grade": true,
     "grade_id": "cell-9141ec6c68600cb3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "def test_13()\n",
    "  spambase = read_sparse_data_from_csv \"spambase\"\n",
    "  zspambase = z_normalize spambase\n",
    "\n",
    "  assert_in_delta 0.27, spambase[\"data\"].first[\"features\"][\"word_freq_our\"], 1e-5\n",
    "  assert_in_delta -0.628106690674003, zspambase[\"data\"].first[\"features\"][\"word_freq_our\"], 1e-5\n",
    "\n",
    "  assert_in_delta 607.0, spambase[\"data\"].first[\"features\"][\"capital_run_length_total\"], 1e-5\n",
    "  assert_in_delta 0.53386, zspambase[\"data\"].first[\"features\"][\"capital_run_length_total\"], 1e-5\n",
    "end\n",
    "\n",
    "test_13()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 (10 Points)\n",
    "\n",
    "Change your ```LinearRegression``` implementation from  [Assignment 5](../assignment-5/assignment-5.ipynb) to implement regularization. The new implementation requires a value for $\\lambda$. The regularization objective function for linear regression in a mini-batch is as follows:\n",
    "\n",
    "# $L(w,X) = \\frac{\\lambda}{2} \\left\\lVert w \\right\\rVert ^ 2 + \\frac{1}{n} \\sum_{i} \\frac{1}{2} \\left(f(w,x_i) - y_i\\right) ^ 2$\n",
    "\n",
    "where ```reg_param``` corresponds to $\\lambda$ in the formula above.\n",
    "\n",
    "Note that there is no $\\frac{1}{n}$ in front of the regularizer penalty. The ```predict``` and ```adjust``` methods have been provided for you. \n",
    "\n",
    "Hint: Use ```dot``` and ```norm``` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4b9f790f64f8c5a40a0dd0bee58d2b39",
     "grade": false,
     "grade_id": "cell-51eeafda9ff5400f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":func"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegressionModelL2\n",
    "  def initialize reg_param\n",
    "    @reg_param = reg_param\n",
    "  end\n",
    "\n",
    "  def predict row, w\n",
    "    x = row[\"features\"]    \n",
    "    yhat = dot(w, x)\n",
    "  end\n",
    "  \n",
    "  def adjust w\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].nan? or w[k].infinite?}\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].abs > 1e5 }\n",
    "  end\n",
    "  \n",
    "  def func data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    l = 0.0\n",
    "    data.each do |x|\n",
    "      l += (predict(x, w) - x[\"label\"]) ** 2\n",
    "    end\n",
    "    return 0.5 * @reg_param * norm(w) ** 2 + l / (2.0 * data.size)\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3ab21a3e07e8524b0233e8ab667473f0",
     "grade": true,
     "grade_id": "cell-d0ce9f8960464aaa",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "def test_21()\n",
    "  m = LinearRegressionModelL2.new 0.0\n",
    "  w = {\"x1\" => 1.0, \"x2\" => -3.0}\n",
    "  x = [\n",
    "      {\"features\" => {\"x1\" => 0.7, \"x2\" => -0.3}, \"label\" => 0.97},\n",
    "      {\"features\" => {\"x1\" => -2.7, \"x2\" => -1.3}, \"label\" => -1.0}  \n",
    "  ]\n",
    "  \n",
    "  e1 = 0.19845\n",
    "  assert_in_delta e1, m.func(x[0,1], w), 1e-2, \"1\"\n",
    "  \n",
    "  e2 = 2.42\n",
    "  assert_in_delta e2, m.func(x[1,1], w), 1e-2, \"2\"    \n",
    "  assert_in_delta (e1 + e2) / 2.0, m.func(x, w), 1e-2, \"3\"  \n",
    "\n",
    "  assert_in_delta 3.1622776602, norm(w), 1e-2, \"4\"\n",
    "  m2 = LinearRegressionModelL2.new 1.7\n",
    "  assert_in_delta 9.809225, m2.func(x, w), 1e-2, \"5\"\n",
    "end\n",
    "\n",
    "test_21()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 (10 Points)\n",
    "\n",
    "Implement the gradient for the regularized linear regression using the above objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1e36b3be7098e5219b4279bcea6f10e4",
     "grade": false,
     "grade_id": "cell-fc654986b9148891",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":grad"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegressionModelL2\n",
    "  def grad data, w\n",
    "    g = Hash.new {|h,k| h[k] = 0.0}\n",
    "    # BEGIN YOUR CODE\n",
    "    data[0][\"features\"].each_key do |v|\n",
    "      gd = 0.0\n",
    "      data.each do |x|\n",
    "        gd += x[\"features\"][v] == nil ? 0.0 : (predict(x, w)-x[\"label\"]) * x[\"features\"][v]\n",
    "      end\n",
    "      g[v] = @reg_param * w[v] + gd / data.size.to_f\n",
    "    end\n",
    "    #END YOUR CODE\n",
    "    return g\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "65f2593fdbd24f407adf2daf10e369e9",
     "grade": true,
     "grade_id": "cell-ac6e14131548a546",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "def test_22()\n",
    "  m = LinearRegressionModelL2.new 0.0\n",
    "  w = {\"x1\" => 1.0, \"x2\" => -3.0}\n",
    "  x = [\n",
    "      {\"features\" => {\"x1\" => 0.7, \"x2\" => -0.3}, \"label\" => 0.97},\n",
    "      {\"features\" => {\"x1\" => -2.7, \"x2\" => -1.3}, \"label\" => -1.0}  \n",
    "  ]\n",
    "  \n",
    "  g1_1 = 0.441\n",
    "  assert_in_delta g1_1, m.grad(x[0,1], w)[\"x1\"], 1e-2, \"1\"\n",
    "  \n",
    "  g2_1 = -5.94\n",
    "  assert_in_delta g2_1, m.grad(x[1,1], w)[\"x1\"], 1e-2, \"2\"    \n",
    "  assert_in_delta (g1_1 + g2_1) / 2.0, m.grad(x, w)[\"x1\"], 1e-2, \"3\"  \n",
    "\n",
    "  m2 = LinearRegressionModelL2.new 1.7\n",
    "  assert_in_delta -1.0495, m2.grad(x, w)[\"x1\"], 1e-2, \"5\"\n",
    "end\n",
    "\n",
    "test_22()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 (10 Points)\n",
    "\n",
    "Implement a function that calculates the Root Mean Squared Error (RMSE) for a given dataset for a prediction model and weights.\n",
    "\n",
    "RMSE is defined as follows:\n",
    "\n",
    "# $e = \\sqrt{\\frac{\\sum_{i=1}^N{ \\left( \\hat{y} - y \\right) ^ 2 }}{N}}$\n",
    "\n",
    "where $N$ is the number of examples in the dataset.\n",
    "\n",
    "Hint: Use the ```mean``` function in the assignment library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3c69bb947bf945ea48b0ffdb5aff9849",
     "grade": false,
     "grade_id": "cell-890117c7468edfd7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def score_regression_model_rmse(data, weights, model)\n",
    "  # BEGIN YOUR CODE\n",
    "  err = []\n",
    "  n = data.size()\n",
    "  data.each do |row|\n",
    "    y_hat = model.predict(row, weights)\n",
    "    err.push ((y_hat - row[\"label\"]) ** 2.0)   \n",
    "  end\n",
    "  return Math.sqrt(mean(err))\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f77d224a43112b93e71344b87728d0ef",
     "grade": true,
     "grade_id": "cell-76971a96370a4069",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "def test_23()\n",
    "  m = LinearRegressionModelL2.new 0.0\n",
    "  w = {\"x1\" => 1.0, \"x2\" => -3.0}\n",
    "  x = [\n",
    "      {\"features\" => {\"x1\" => 0.7, \"x2\" => -0.3}, \"label\" => 0.97},\n",
    "      {\"features\" => {\"x1\" => -2.7, \"x2\" => -1.3}, \"label\" => -1.0}  \n",
    "  ]\n",
    "  \n",
    "  e1 = (((0.7 + -3 * -0.3) - 0.97) ** 2)\n",
    "  e2 = (((-2.7 + -3.0 * -1.3) - -1.0) ** 2)\n",
    "  \n",
    "  rmse = Math.sqrt((e1 + e2) / 2.0)\n",
    "  assert_in_delta rmse, score_regression_model_rmse(x, w, m), 1e-2, \"3\"  \n",
    "end\n",
    "\n",
    "test_23()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1 (10 Points)\n",
    "\n",
    "Using a small provided dataset, shown below, we will investigate model complexity. First, implement a _polynomial_ feature representation. \n",
    "\n",
    "Call the bias feature \"1\" for this part. For a dataset with two features, $x_1$ and $x_2$, a polynomial representation of degree 0 is as follows:\n",
    "\n",
    "# $\\phi(x, k = 0) = \\left( 1 \\right)$\n",
    "\n",
    "degree 1:\n",
    "\n",
    "# $\\phi(x, k = 1) = \\left( 1, x_1, x_2 \\right)$\n",
    "\n",
    "degree 2: \n",
    "\n",
    "# $\\phi(x, k = 2) = \\left( 1, x_1, x_2, x_1^2, x_2^2, x_1 x_2  \\right)$\n",
    "\n",
    "and more generally, for degree $k$:\n",
    "\n",
    "# $\\phi(x, k) = \\left(1, x_1 \\phi(x,k-1), x_2 \\phi(x,k-1) \\right)$\n",
    "\n",
    "For your convenience, the function ```poly_features``` emits, for degree $k$, the names of the features to be multiplied. After generating the features, apply ```z_normalize``` to only the newly added features (i.e., not the original features or the bias).\n",
    "\n",
    "Note: You may notice that the dataset we plan to use only has one feature and therefore the above seems overly complex. Don't worry, we will see this again. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "649f9b716eb117031e7c8bc730d7f635",
     "grade": false,
     "grade_id": "cell-7be23edc4f33cc01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = read_sparse_data_from_csv \"polydata\"\n",
    "polydata = create_polynomial_features data, 1\n",
    "x1 = polydata[\"data\"].collect {|r| r[\"features\"][\"x1\"]}\n",
    "x2 = polydata[\"data\"].collect {|r| r[\"label\"]}\n",
    "puts \"Polydata Regression Dataset\"\n",
    "Daru::DataFrame.new({x1: x1, x2: x2})\n",
    ".plot(type: :scatter, x: :x1, y: :x2) do |plot, diagram|\n",
    "  plot.x_label \"X1\"\n",
    "  plot.y_label \"Label\"\n",
    "  plot.legend false\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7c50bd053e4cc638aa998e199af52716",
     "grade": false,
     "grade_id": "cell-7381e666dbe11a64",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def poly_features features, degree\n",
    "  poly_features = [\"1\"]\n",
    "\n",
    "  degree.times do |i|\n",
    "    poly_features += poly_features.flat_map do |x_prev|\n",
    "      features.reject {|x| x == \"1\" or x == \"bias\"}.collect do |x|\n",
    "        [x, x_prev.split(\"*\")].flatten.sort.join(\"*\")\n",
    "      end\n",
    "    end\n",
    "    poly_features.uniq!\n",
    "  end\n",
    "  poly_features.collect {|k| k.gsub /^1\\*([^\\*]*)$/, '\\1'}\n",
    "end\n",
    "\n",
    "poly_features [\"x1\", \"x2\"], 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8601239861b56daa6c54c2e4a2844e21",
     "grade": false,
     "grade_id": "cell-f3c20b1351e644d5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_polynomial_features dataset, degree\n",
    "  polydataset = dataset.clone\n",
    "  features = poly_features dataset[\"features\"], degree\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b0854f5022da663273a14ff8e2519a9",
     "grade": true,
     "grade_id": "cell-1ecdd36c24cd8684",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_31()\n",
    "  data = read_sparse_data_from_csv \"polydata\"\n",
    "  assert_in_delta 12.8132, data[\"data\"].first[\"features\"][\"x1\"], 1e-2, \"1\"\n",
    "  \n",
    "  polydata = create_polynomial_features data, 3\n",
    "  \n",
    "  xp = polydata[\"data\"].first[\"features\"]\n",
    "  assert_in_delta 12.8132, xp[\"x1\"], 1e-2, \"2: Does not normalize original features\"\n",
    "  assert_in_delta -0.905, xp[\"1*x1*x1\"], 1e-2, \"3: Applies normalization to new features\"\n",
    "  assert_in_delta -0.827, xp[\"1*x1*x1*x1\"], 1e-2, \"4: Applies normalization to new features\"\n",
    "end\n",
    "\n",
    "test_31()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2 (5 Points)\n",
    "\n",
    "Let's fit this dataset with different polynomial degrees. First, let's see how well linear regression fits the training data. \n",
    "\n",
    "Implement a training function that, given a training and testing dataset, trains the model using mini-batch SGD and returns the RMSE error value on both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc3a9a3243de06953072e87341a085e9",
     "grade": false,
     "grade_id": "cell-c62959d2c5c0fccb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train(sgd, obj, w, train_set, test_set, num_epoch = 100, batch_size = 20)\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return [train_rmse, test_rmse]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2a9342a03f4792e343e3616d9f234c67",
     "grade": true,
     "grade_id": "cell-f472d45ada3dee64",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_32()\n",
    "  data = read_sparse_data_from_csv \"polydata\"\n",
    "  polydata = create_polynomial_features data, 1\n",
    "  x1 = polydata[\"data\"].collect {|r| r[\"features\"][\"x1\"]}\n",
    "  x2 = polydata[\"data\"].collect {|r| r[\"label\"]}\n",
    "  \n",
    "  w = Hash.new {|h,k| h[k] = 0.0}\n",
    "  lr = 1e-3\n",
    "  obj = LinearRegressionModelL2.new 0.0\n",
    "  sgd = StochasticGradientDescent.new obj, w, lr\n",
    "\n",
    "  train_set = polydata\n",
    "  test_set = polydata\n",
    "  train_rmse, test_rmse = train(sgd, obj, w, train_set, test_set, num_epoch = 100, batch_size = 20)\n",
    "  assert_true train_rmse < 2, \"1\"\n",
    "  assert_true test_rmse < 2, \"2\"\n",
    "  assert_true train_rmse > 0, \"3\"\n",
    "  assert_true test_rmse > 0, \"4\"\n",
    "  assert_in_delta train_rmse, test_rmse, 1e-5, \"5\"\n",
    "end\n",
    "\n",
    "test_32_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3 (10 Points)\n",
    "\n",
    "Implement a simplified version of Gaussian Complexity. Observe that as model complexity increases, test error worsens.\n",
    "\n",
    "In this simplification, we will compute the average loss of a randomly permuted datasets. Let $H(X,Y)$ be the loss on the training set on a function trained on input examples $x_i\\in X$ with labels $y_i\\in Y$. Permute the training labels as $y^\\prime_i = g y_i$ where $g ~ N(0,1)$ is sampled from a normal distribution with mean 0 and standard deviation 1. Compute the following Gaussian Complexity:\n",
    "\n",
    "# $R_G(X,H) = -\\frac{1}{K} \\sum_k H(X,Y^\\prime) $\n",
    "\n",
    "which, in words, is the average of $K$ separate trainings each with a randomly permuted label. We use negative RMSE here to indicate that a more complex model should be more sensitive to permutation and therefore its loss should be lower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e00a578b719986bb150dd39fd6620d71",
     "grade": false,
     "grade_id": "cell-512cc2e8d8e39163",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_complexity(dataset, obj)\n",
    "  rng = Distribution::Normal.rng(0,1, 293891)\n",
    "  lr = 1e-2\n",
    "  tr_rmses = []\n",
    "  te_rmses = []  \n",
    "  norms = []\n",
    "\n",
    "  100.times do |i|\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end  \n",
    "  result = [mean(tr_rmses), mean(norms), mean(te_rmses)]\n",
    "  puts result.join(\"\\t\")\n",
    "  result\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b638fdd8d2459a980b01e66f75786286",
     "grade": true,
     "grade_id": "cell-a6fa85464f66be18",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_33()\n",
    "  stats = Hash.new {|h,k| h[k] = []}\n",
    "  \n",
    "  8.times do |i|\n",
    "    data = read_sparse_data_from_csv \"polydata\"\n",
    "    polydata = create_polynomial_features data, i\n",
    "    obj = LinearRegressionModelL2.new reg\n",
    "    tr_rmse, w_norm, te_rmse = gaussian_complexity(polydata, obj)\n",
    "    \n",
    "    stats[:degree] << i\n",
    "    stats[:train_rmse] << tr_rmse    \n",
    "    stats[:test_rmse] << te_rmse\n",
    "    stats[:complexity] << -tr_rmse\n",
    "  end\n",
    "  tr_rmse = stats[:train_rmse]\n",
    "  assert_true(tr_rmse[0] > 0.0)\n",
    "  assert_true(tr_rmse[0] > tr_rmse[1])\n",
    "  assert_true(tr_rmse[1] > tr_rmse[2])\n",
    "  assert_true(tr_rmse[2] > tr_rmse[3])\n",
    "  assert_true(tr_rmse[2] < 10.0)\n",
    "  \n",
    "  te_rmse = stats[:test_rmse]\n",
    "  assert_true(te_rmse[0] > 0.0)\n",
    "  assert_true(te_rmse[0] < te_rmse[1])\n",
    "  assert_true(te_rmse[1] < te_rmse[2])\n",
    "  assert_true(te_rmse.last < 10.0)\n",
    "  \n",
    "  z_plot = Nyaplot::Plot.new\n",
    "  z_plot.x_label(\"Model Complexity\").y_label(\"Test RMSE\")\n",
    "  z_plot.add(:line, stats[:complexity], stats[:test_rmse]).color(:black)\n",
    "  z_plot.show()  \n",
    "end\n",
    "test_33()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.4 (5 points)\n",
    "\n",
    "Does regularization reduce the Gaussian Complexity? Copy ```test_33``` above and modify it to select a fixed value for the polynomial degree, say $k=5$. Validate that both norm and complexity decreases as you increase the regularization parameter. Due to limitations in SGD, some large regularization values may cause the trainer to diverge. Try adjusting the learning rate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "61e3c314d54cc65a5d6754177b5adb64",
     "grade": false,
     "grade_id": "cell-c38f15b4d5b724ff",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def complexity_vs_norm()\n",
    "  stats = Hash.new {|h,k| h[k] = []}\n",
    "  data = read_sparse_data_from_csv \"polydata\"\n",
    "\n",
    "  [0.0, 0.1, 0.5, 1.0, 1.5, 2.0, 5.0, 10.0, 15.0, 100.0].each do |reg|\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "    stats[:regularizer] << reg\n",
    "    stats[:train_rmse] << tr_rmse    \n",
    "    stats[:test_rmse] << te_rmse\n",
    "    stats[:norms] << w_norm    \n",
    "    stats[:complexity] << -tr_rmse\n",
    "  end\n",
    "  \n",
    "  return stats\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "43fdffa7ef6c8e4ebc8064bb96abc4aa",
     "grade": true,
     "grade_id": "cell-f31f60f16e70ba4e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_34()\n",
    "  stats = complexity_vs_norm()\n",
    "  \n",
    "  assert_true(stats[:train_rmse].all? {|t| t > 0 and t < 5})\n",
    "  assert_true(stats[:test_rmse].all? {|t| t > 0 and t < 5})  \n",
    "  assert_true(stats[:norms].all? {|t| t > 0 and t < 10})    \n",
    "  z_plot = Nyaplot::Plot.new\n",
    "  z_plot.x_label(\"Weight Norm\").y_label(\"Model Complexity\")\n",
    "  z_plot.add(:line, stats[:norms], stats[:complexity]).color(:black)\n",
    "  z_plot.show()  \n",
    "end\n",
    "\n",
    "test_34()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1 (10 Points)\n",
    "\n",
    "Moving on to classification, implement L2 regularization for Logisitic Regression. This should follow closely what you did in Question 2.X above. \n",
    "\n",
    "Use the Log Loss formulation, $\\log(1 + \\exp(-y\\cdot \\hat{y}))$ when calculating the objective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "01b379b0d26f2e1b8116404f8cd38a1d",
     "grade": false,
     "grade_id": "cell-07e2446bcbbb508d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionModelL2\n",
    "  def initialize reg_param\n",
    "    @reg_param = reg_param\n",
    "  end\n",
    "\n",
    "  def predict row, w\n",
    "    x = row[\"features\"]    \n",
    "    1.0 / (1 + Math.exp(-dot(w, x)))\n",
    "  end\n",
    "  \n",
    "  def adjust w\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].nan? or w[k].infinite?}\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].abs > 1e5 }\n",
    "  end\n",
    "  \n",
    "  def func data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bcbf9888c35631621f59f3637f840e7d",
     "grade": true,
     "grade_id": "cell-0a9f2ab984b41ba5",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "def test_41()\n",
    "  m = LogisticRegressionModelL2.new 0.0\n",
    "  w = {\"x1\" => 1.0, \"x2\" => -3.0}\n",
    "  x = [\n",
    "      {\"features\" => {\"x1\" => 0.7, \"x2\" => -0.3}, \"label\" => 1.0},\n",
    "      {\"features\" => {\"x1\" => -2.7, \"x2\" => -1.3}, \"label\" => -1.0}  \n",
    "  ]\n",
    "  \n",
    "  e1 = 0.1839007409\n",
    "  assert_in_delta e1, m.func(x[0,1], w), 1e-2, \"1\"\n",
    "  \n",
    "  e2 = 1.4632824673\n",
    "  assert_in_delta e2, m.func(x[1,1], w), 1e-2, \"2\"    \n",
    "  assert_in_delta (e1 + e2) / 2.0, m.func(x, w), 1e-2, \"3\"  \n",
    "\n",
    "  assert_in_delta 3.1622776602, norm(w), 1e-2, \"4\"\n",
    "  m2 = LogisticRegressionModelL2.new 1.7\n",
    "  assert_in_delta 9.3235916041, m2.func(x, w), 1e-2, \"5\"\n",
    "end\n",
    "\n",
    "test_41()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2 (10 Points)\n",
    "\n",
    "Implement the gradient for L2 regularized Logisitic Regression. As in Assignment 5, use the 0 / 1 version of the loss to simplify the derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e810ebcba9bd6d75f820c4a8c33ef7b4",
     "grade": false,
     "grade_id": "cell-66a9fef971a48fd7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionModelL2\n",
    "  def grad data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "    return g\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a51733c8f16f893b74a8f0fbfc860668",
     "grade": true,
     "grade_id": "cell-e964eeb5016b4f19",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "def test_42()\n",
    "  m = LogisticRegressionModelL2.new 0.0\n",
    "  w = {\"x1\" => 1.0, \"x2\" => -3.0}\n",
    "  x = [\n",
    "      {\"features\" => {\"x1\" => 0.7, \"x2\" => -0.3}, \"label\" => 1.0},\n",
    "      {\"features\" => {\"x1\" => -2.7, \"x2\" => -1.3}, \"label\" => -1.0}  \n",
    "  ]\n",
    "  \n",
    "  g1_1 = -0.1175871304\n",
    "  assert_in_delta g1_1, m.grad(x[0,1], w)[\"x1\"], 1e-2, \"1\"\n",
    "  \n",
    "  g2_1 =  -2.0750169154\n",
    "  assert_in_delta g2_1, m.grad(x[1,1], w)[\"x1\"], 1e-2, \"2\"    \n",
    "  assert_in_delta (g1_1 + g2_1) / 2.0, m.grad(x, w)[\"x1\"], 1e-2, \"3\"  \n",
    "\n",
    "  m2 = LogisticRegressionModelL2.new 1.7\n",
    "  assert_in_delta 1.0 * 1.7 + (g1_1 + g2_1) / 2.0, m2.grad(x, w)[\"x1\"], 1e-2, \"5\"\n",
    "end\n",
    "\n",
    "test_42()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3 (2 points)\n",
    "\n",
    "Implement a function that will score your logistic regression model and return an array of pairs of (score, class label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6cdc98a4f86b9aadc9d426a892e1836e",
     "grade": false,
     "grade_id": "cell-75eafcbbd387d5df",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def score_binary_classification_model(data, weights, model)\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return scores\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fe77a6289e41974c4ed628ab15448b57",
     "grade": true,
     "grade_id": "cell-f2ea4529b7661a32",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "def test_43()\n",
    "  m = LogisticRegressionModelL2.new 888.0\n",
    "  w = {\"x1\" => 1.0, \"x2\" => -3.0}\n",
    "  x = [\n",
    "      {\"features\" => {\"x1\" => 0.7, \"x2\" => -0.3}, \"label\" => 1.0},\n",
    "      {\"features\" => {\"x1\" => -2.7, \"x2\" => -1.3}, \"label\" => 0.0}  \n",
    "  ]\n",
    "  \n",
    "  e1 = 0.8320183851\n",
    "  e2 = 0.7685247835\n",
    "  \n",
    "  scores = score_binary_classification_model(x, w, m)\n",
    "  assert_in_delta e1, scores[0][0], 1e-2, \"1\"\n",
    "  assert_in_delta e2, scores[1][0], 1e-2, \"2\"  \n",
    "  assert_in_delta 1.0, scores[0][1], 1e-2, \"3\"\n",
    "  assert_in_delta 0.0, scores[1][1], 1e-2, \"4\"  \n",
    "end\n",
    "\n",
    "test_43()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.1 (10 Points)\n",
    "\n",
    "Given an array of pairs of score and class label (0,1), calculate the AUC metric. It is not necessary to draw the curve, but you are welcome to do that. Assume scores are not sorted.\n",
    "\n",
    "Recall the definition of AUC as either the area under the ROC curve or the probability of mis-ranking a positive example. Choose one of these methods for the implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f484b5c54fa762cff8e57ab60e49c64d",
     "grade": false,
     "grade_id": "cell-1768e36e21c260fb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_auc_only(scores)\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return auc\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54872efa78ff7e94e643624cdec1f61d",
     "grade": true,
     "grade_id": "cell-75993046270a7c4b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_51()\n",
    "  good_model = [[0.9, 1], [0.89, 1], [0.7, 0], [0.8, 1], [0.8, 0], [0.7, 1], [0.6, 0], [0.5, 0], [0.1, 0]]\n",
    "  assert_true(calc_auc_only(good_model) > 0.8)\n",
    "  assert_true(calc_auc_only(good_model) < 1)\n",
    "  \n",
    "  srand(777)\n",
    "  ok_model = Array.new(100) {|i| [100 - i, (rand < (100 - i) / 100.0) ? 1 : 0] }\n",
    "  ok_auc = calc_auc_only(ok_model)\n",
    "  assert_in_delta(0.8631239935587761, ok_auc, 1e-3)\n",
    "  \n",
    "  bad_model = Array.new(1000) {|i| [1000 - i, rand < 0.5 ? 1 : 0] }\n",
    "  bad_auc = calc_auc_only(bad_model)\n",
    "  assert_in_delta(0.5, bad_auc, 5e-2)\n",
    "\n",
    "end\n",
    "\n",
    "test_51()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.2 (10 Points)\n",
    "\n",
    "The following dataset has _irrelevant features_. Find them and use regularization to control them. \n",
    "\n",
    "Implement a training method that trains a logistic regression model and returns training and testing AUC values. This follows closely question 3.2 above. Next, fill in the driver code that trains the model for each regularization value and populates an array of training AUC, testing AUC, and weight vector norm values.\n",
    "\n",
    "Hint: The weights for regularization parameter are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "81f003c01f00e981ca2ee596a0274bdc",
     "grade": false,
     "grade_id": "cell-e345def53c509c4b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_logistic_regression(sgd, obj, w, train_set, test_set, num_epoch = 100, batch_size = 20)\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return [train_auc, test_auc]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8f5c7cbf909195e93636f4e0c120c101",
     "grade": false,
     "grade_id": "cell-6776506c206e8278",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test_logistic_regularizers(corner)\n",
    "  stats = Hash.new {|h,k| h[k] = Array.new}\n",
    "  [0.0, 0.01, 0.05, 0.1, 0.15, 0.2, 0.5, 1.0, 10.0].each do |reg|\n",
    "    tr_aucs = []\n",
    "    te_aucs = []\n",
    "    w_norms = []\n",
    "\n",
    "    cross_validate corner, 2 do |tr, te, fold|\n",
    "      # BEGIN YOUR CODE\n",
    "      raise NotImplementedError.new()\n",
    "      #END YOUR CODE\n",
    "      puts w if fold == 0\n",
    "    end\n",
    "    puts [reg, mean(w_norms), mean(tr_aucs), mean(te_aucs), stdev(te_aucs)].join(\"\\t\")\n",
    "    stats[:reg] << reg\n",
    "    stats[:tr_aucs] << mean(tr_aucs)\n",
    "    stats[:w_norms] << mean(w_norms)\n",
    "    stats[:te_aucs] << mean(te_aucs)    \n",
    "  end\n",
    "  \n",
    "  return stats\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a973dc2334292d207bdfd0ef84ef5378",
     "grade": true,
     "grade_id": "cell-3466fb23ace898e9",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_52()\n",
    "  corner = z_normalize(read_sparse_data_from_csv(\"corner\"))\n",
    "  corner[\"data\"].first\n",
    "  \n",
    "  stats = test_logistic_regularizers(corner)\n",
    "  assert_true(stats[:tr_aucs].all? {|a| a > 0.7 and a < 1.0}, \"1\")\n",
    "  assert_true(stats[:te_aucs].all? {|a| a > 0.7 and a < 1.0}, \"2\")\n",
    "  assert_true(stats[:w_norms][0] > stats[:w_norms][6], \"3\")\n",
    "  assert_true(stats[:w_norms][6] > stats[:w_norms].last, \"4\")  \n",
    "  Daru::DataFrame.new stats\n",
    "end\n",
    "\n",
    "test_52()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.3 (5 Points)\n",
    "\n",
    "Make the function below return an array of feature names you think are irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b6f35fb66ab0d03261c09b9e8f4e5be0",
     "grade": false,
     "grade_id": "cell-1db59fbc2e27be51",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def guess_irrelevant_features()\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return answer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b1c5d71ff4eb824db310dd32f352f881",
     "grade": true,
     "grade_id": "cell-f6112841216e9542",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "corner = read_sparse_data_from_csv(\"corner\")\n",
    "\n",
    "t53_answer = guess_irrelevant_features()\n",
    "assert_true(t53_answer.is_a?(Array))\n",
    "assert_false(t53_answer.empty?)\n",
    "assert_false((corner[\"features\"] & t53_answer).empty?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.5.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
