{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Gradient Descent and Maximum Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if(window['d3'] === undefined ||\n",
       "   window['Nyaplot'] === undefined){\n",
       "    var path = {\"d3\":\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\",\"downloadable\":\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\"};\n",
       "\n",
       "\n",
       "\n",
       "    var shim = {\"d3\":{\"exports\":\"d3\"},\"downloadable\":{\"exports\":\"downloadable\"}};\n",
       "\n",
       "    require.config({paths: path, shim:shim});\n",
       "\n",
       "\n",
       "require(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\n",
       "\n",
       "\tvar script = d3.select(\"head\")\n",
       "\t    .append(\"script\")\n",
       "\t    .attr(\"src\", \"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\")\n",
       "\t    .attr(\"async\", true);\n",
       "\n",
       "\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\n",
       "\n",
       "\n",
       "\t    var event = document.createEvent(\"HTMLEvents\");\n",
       "\t    event.initEvent(\"load_nyaplot\",false,false);\n",
       "\t    window.dispatchEvent(event);\n",
       "\t    console.log('Finished loading Nyaplotjs');\n",
       "\n",
       "\t};\n",
       "\n",
       "\n",
       "});});\n",
       "}\n"
      ],
      "text/plain": [
       "\"if(window['d3'] === undefined ||\\n   window['Nyaplot'] === undefined){\\n    var path = {\\\"d3\\\":\\\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\\\",\\\"downloadable\\\":\\\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\\\"};\\n\\n\\n\\n    var shim = {\\\"d3\\\":{\\\"exports\\\":\\\"d3\\\"},\\\"downloadable\\\":{\\\"exports\\\":\\\"downloadable\\\"}};\\n\\n    require.config({paths: path, shim:shim});\\n\\n\\nrequire(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\\n\\n\\tvar script = d3.select(\\\"head\\\")\\n\\t    .append(\\\"script\\\")\\n\\t    .attr(\\\"src\\\", \\\"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\\\")\\n\\t    .attr(\\\"async\\\", true);\\n\\n\\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\\n\\n\\n\\t    var event = document.createEvent(\\\"HTMLEvents\\\");\\n\\t    event.initEvent(\\\"load_nyaplot\\\",false,false);\\n\\t    window.dispatchEvent(event);\\n\\t    console.log('Finished loading Nyaplotjs');\\n\\n\\t};\\n\\n\\n});});\\n}\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require './assignment_lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 (5 points)\n",
    "\n",
    "Let's implement a test function for the gradient descent optimizer, a 3D simple parabola. All gradient-optimized trainers are implemented as a objective function. The follow the same basic pattern:\n",
    "\n",
    "```ruby\n",
    "class MyGradientLearnableModel\n",
    "  def func x, w\n",
    "    #Returns the value of the objective function, \n",
    "    #  summing across all examples in x\n",
    "  end\n",
    "  def grad x, w\n",
    "    #Returns a hash of derivative values for each variable in w,\n",
    "    # gradient is summed across all examples in x\n",
    "    dw = {\"0\" => (w[\"0\"] - 1), \"1\" => (w[\"1\"] - 2)}\n",
    "  end\n",
    "  def adjust w\n",
    "    # Applies any problem-specific alterations to w\n",
    "  end\n",
    "end\n",
    "```\n",
    "\n",
    "Now, let's implement a Parabola objective function which does not depend on the data at all. It is defined as follows:\n",
    "\n",
    "### $L(w) = \\frac{1}{2}\\left( \\left(w_{0} - 1\\right)^2 + \\left(w_{0} - 2\\right)^2 \\right)$\n",
    "\n",
    "Implement the ```func``` method for the loss function, $L(w)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54115f66272f6d5339cda445dcca261d",
     "grade": false,
     "grade_id": "cell-f52748ce9cfc537f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ParabolaObjective\n",
    "  def func x, w\n",
    "    # BEGIN YOUR CODE\n",
    "    return 0.5 * ((w[\"0\"] - 1) ** 2 + (w[\"1\"] - 2) ** 2) \n",
    "  end\n",
    "  def adjust w\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f730730805da8370e5bbb243d401a989",
     "grade": true,
     "grade_id": "cell-91665d329bcaad34",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "t1 = ParabolaObjective.new\n",
    "assert_in_delta(0.0, t1.func([], {\"0\" => 1.0, \"1\" => 2.0}), 1e-3)\n",
    "assert_in_delta(0.5, t1.func([], {\"0\" => 1.0, \"1\" => 1.0}), 1e-3)\n",
    "assert_in_delta(0.5, t1.func([], {\"0\" => 1.0, \"1\" => 3.0}), 1e-3)\n",
    "assert_in_delta(2.5, t1.func([], {\"0\" => 3.0, \"1\" => 1.0}), 1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2 (5 Points)\n",
    "\n",
    "Implement the gradient function for $L(w)$. It evaluates the gradient for the value of $x$ for each dimension of $w$. In this simple case, $L(w)$ does not depend on $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "336415156704e9b2ea16ae3940b2e032",
     "grade": false,
     "grade_id": "cell-b5c9699ed1ed88fa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":grad"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ParabolaObjective\n",
    "  def grad x, w\n",
    "    # BEGIN YOUR CODE\n",
    "    return {\"0\" => w[\"0\"] - 1, \"1\" => w[\"1\"] - 2};\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e9b03b5652d757511bb12c6642715b2d",
     "grade": true,
     "grade_id": "cell-aaa3cd8bce5fe79b",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "t2 = ParabolaObjective.new\n",
    "w2_1 = t2.grad([], {\"0\" => 3.0, \"1\" => 7.0})\n",
    "assert_in_delta(2.0, w2_1[\"0\"], 1e-3)\n",
    "assert_in_delta(5.0, w2_1[\"1\"], 1e-3)\n",
    "\n",
    "w2_2 = t2.grad([], {\"0\" => -3.0, \"1\" => -7.0})\n",
    "assert_in_delta(-4.0, w2_2[\"0\"], 1e-3)\n",
    "assert_in_delta(-9.0, w2_2[\"1\"], 1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 (1 Point)\n",
    "\n",
    "\n",
    "Implement gradient descent for any objective function class. Your function must provide a callback which we will use to monitor its performance and possibly to halt execution. A simple example illustrating the callback is as follows:\n",
    "\n",
    "```ruby\n",
    "def gradient_descent_example dataset, w, obj, learning_rate, tol, max_iter, &block\n",
    "    iter = 0\n",
    "    until converged(w_last, w) do\n",
    "        w_last = w\n",
    "        loss = calc_loss(w)\n",
    "        w = update(w)\n",
    "        w = adjust(w)\n",
    "        break unless yield w, iter, loss\n",
    "        iter += 1\n",
    "    end\n",
    "    \n",
    "    return w\n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "There are three main parts to the algorithm above:\n",
    "1. Convergence is based on the norm of the difference of the weight vectors\n",
    "1. The norm can be calculated as the dot product of two vectors: $||w|| = w \\cdot w$\n",
    "1. Once the loss and gradient functions are calculated, we will update the values of each weight\n",
    "\n",
    "In this first part, implement the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cd831dd58d1fca904a54df797cbaf64b",
     "grade": false,
     "grade_id": "cell-7af25ba96bfb8ab2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":dot"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implement the error function given a weight vector, w\n",
    "def dot x, w\n",
    "  # BEGIN YOUR CODE\n",
    "    sum = 0.0\n",
    "    \n",
    "    if !(x.empty? or w.empty?)\n",
    "      x.each do |k, v|\n",
    "          if w.has_key?(k)\n",
    "              sum += v * w[k]\n",
    "          end\n",
    "      end\n",
    "    end\n",
    "    \n",
    "    return sum\n",
    "  #END YOUR CODE\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "729b2f04141c6e728d5a50c168864202",
     "grade": true,
     "grade_id": "cell-6e56a41a0ad960bf",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "assert_in_delta 6.0, dot({\"a\" => 2.0}, {\"a\" => 3.0}), 1e-6\n",
    "assert_in_delta 6.0, dot({\"a\" => 2.0}, {\"a\" => 3.0, \"b\" => 4.0}), 1e-6\n",
    "assert_equal 0.0, dot({}, {})\n",
    "assert_equal 0.0, dot({\"a\" => 1.0}, {\"b\" => 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 (1 Point)\n",
    "Implement the L2 norm for a vector, represented by a hash. Hint: use ```dot```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "87e05e64c2e89f801ccf85a4b394f421",
     "grade": false,
     "grade_id": "cell-4b9adf1cdcc42b60",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":norm"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm w\n",
    "  # BEGIN YOUR CODE\n",
    "  return 0.5 * Math.sqrt(dot(w, w))\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff0e190198d083e0446e990e15f34f59",
     "grade": true,
     "grade_id": "cell-6732c7d27e4f664a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "assert_in_delta 1.0, norm({\"a\" => 1.41421, \"b\" => 1.41421}), 1e-2\n",
    "assert_in_delta 1.0, norm({\"a\" => -1.41421, \"b\" => 1.41421}), 1e-2\n",
    "assert_in_delta 0.0, norm({}), 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 (3 points)\n",
    "Implement a function that updates a weight vector, ```w```, given a gradient vector, ```dw```, and learning rate, ```lr```.  Do not change the original weight vector. Hint: use ```clone```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a759dd69710e353e41a8435530c516cd",
     "grade": false,
     "grade_id": "cell-7353e3fd009c70fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":update_weights"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_weights(w, dw, lr)\n",
    "  # BEGIN YOUR CODE\n",
    "  wTemp = w.clone\n",
    "  wTemp.each do |k, v|\n",
    "    wTemp[k] -= dw[k] * lr\n",
    "  end \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35238b8fa6c3aab46d461bb48957817e",
     "grade": true,
     "grade_id": "cell-7890d2e9cc768dbc",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "assert_in_delta 1.5, update_weights({\"a\" => 1.0}, {\"a\" => -0.25}, 2.0)[\"a\"], 1e-2\n",
    "assert_in_delta 2.5, update_weights({\"a\" => 1.0, \"b\" => 3.0}, {\"a\" => -0.25, \"b\" => 0.25}, 2.0)[\"b\"], 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4 (15 Points)\n",
    "\n",
    "Now, put all these functions together to implement gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "85f39c054fa15db730ee9ef290ecab75",
     "grade": false,
     "grade_id": "cell-54768dfb34dacd23",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent dataset, w, obj, learning_rate, tol, max_iter, &block\n",
    "  # BEGIN YOUR CODE\n",
    "  iter = 0\n",
    "    until converged(w_last, w) do\n",
    "        w_last = w\n",
    "        loss = calc_loss(w)\n",
    "        w = update(w)\n",
    "        w = adjust(w)\n",
    "        break unless yield w, iter, loss\n",
    "        iter += 1\n",
    "    end\n",
    "\n",
    "    return w\n",
    "  #END YOUR CODE\n",
    "  return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8835ef2ae23e4ee43747f8d5d9bfcd16",
     "grade": true,
     "grade_id": "cell-e54d5e8595d96ad4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t4 = ParabolaObjective.new\n",
    "t4_w_init = {\"0\" => 3.0, \"1\" => 7.0}\n",
    "t4_w_goal = {\"0\" => 1.0, \"1\" => 2.0}\n",
    "t4_data = {\"data\" => []}\n",
    "\n",
    "t4_loss = 1.0\n",
    "t4_w = nil\n",
    "gradient_descent t4_data, t4_w_init, t4, 0.1, 0.001, 100 do |w, iter, loss|\n",
    "  t4_loss = loss\n",
    "  t4_w = w\n",
    "end\n",
    "\n",
    "assert_in_delta 0.01, t4_loss, 1e-2\n",
    "assert_in_delta 1.0, t4_w[\"0\"], 1e-1\n",
    "assert_in_delta 2.0, t4_w[\"1\"], 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9cbd3b2dae8f138bbdf3cf952fa4189f",
     "grade": true,
     "grade_id": "cell-208aaf7c7bf7b770",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t4 = ParabolaObjective.new\n",
    "t4_w_init = {\"0\" => 3.0, \"1\" => 7.0}\n",
    "t4_data = {\"data\" => []}\n",
    "\n",
    "t4_total_loss = 0.0\n",
    "t4_iterations = []\n",
    "t4_losses = []\n",
    "gradient_descent t4_data, t4_w_init, t4, 0.1, 0.001, 100 do |w, iter, loss|\n",
    "  t4_total_loss += loss\n",
    "  t4_iterations << iter\n",
    "  t4_losses << t4_total_loss / iter + 1.0\n",
    "end\n",
    "\n",
    "assert_true t4_iterations.size > 30\n",
    "assert_true t4_losses[-1] < 3\n",
    "assert_true t4_losses[-1] > 0\n",
    "assert_true t4_iterations[-1] > 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the cumulative loss per iteration\n",
    "Daru::DataFrame.new({x: t4_iterations, y: t4_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Iteration\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1 (5 Points)\n",
    "\n",
    "Let's learn the parameter of a Bernoulli distribution using the method of maximum likelihood. Consider the following dataset in which we are tossing a biased coin with probability $\\mu$ of returning a success (1). There is an analytical solution for the parameter $\\mu$ given a dataset of successes and trials. Compute this analytical solution. \n",
    "\n",
    "Here the ```label``` field is either 0 or 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "908d76a78e7e78ba9a7ab1f3beed242e",
     "grade": false,
     "grade_id": "cell-5fa56022cb1d93b4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "coin_data = coin_dataset(1000)\n",
    "\n",
    "def q31_binomial_parameter(coin_data)\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0c9b7b7e94293f333e7c9ac657d48493",
     "grade": true,
     "grade_id": "cell-00e4a8fc6cd8141e",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t31_coin_data = coin_dataset(10000)\n",
    "assert_in_delta 0.77, q31_binomial_parameter(t31_coin_data), 5e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2 (5 Points)\n",
    "\n",
    "Now, let's use the maximum likelihood function and gradient descent to find the same parameter value. Define the objective function for a binomial distribution for multiple samples. Remember that the ```label``` is the target value and every example has only one feature, ```bias```. Learn the weight for the ```bias``` feature should converge to $w_{bias} = \\mu$.\n",
    "\n",
    "In this first step, calculate the log likelihood function for the binomial distribution of $n$ examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0af7a42c617bf3ab3f6fb733a292f615",
     "grade": false,
     "grade_id": "cell-92354ef065795ab6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class BinomialModel\n",
    "  def func dataset, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "16e7fc8ff420037c1f90f69378c939be",
     "grade": true,
     "grade_id": "cell-68cf9b7f4c38b214",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t32_model = BinomialModel.new\n",
    "t32_t1 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 0.0}\n",
    "t32_t2 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 1.0}\n",
    "\n",
    "assert_in_delta 1.469677, t32_model.func([t32_t1], {\"bias\" => 0.77}), 1e-3\n",
    "assert_in_delta 0.261365, t32_model.func([t32_t2], {\"bias\" => 0.77}), 1e-3\n",
    "assert_in_delta 1.731041, t32_model.func([t32_t1, t32_t2], {\"bias\" => 0.77}), 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3 (5 Points)\n",
    "\n",
    "Calculate the gradient of the binomial log likelihood function over $n$ examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8fc64cd48d7369e5c88e1262ef0ea11e",
     "grade": false,
     "grade_id": "cell-af9a1550d76082ae",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class BinomialModel\n",
    "  def grad dataset, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "    return g\n",
    "  end\n",
    "  \n",
    "  ## Adjusts the parameter to be within the allowable range\n",
    "  def adjust w\n",
    "    w[\"bias\"] = [[0.001, w[\"bias\"]].max, 0.999].min\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0929e3bf098fe4ae116a9e170a78e80f",
     "grade": true,
     "grade_id": "cell-d87514aa1ef00351",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t32_model = BinomialModel.new\n",
    "t32_t1 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 0.0}\n",
    "t32_t2 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 1.0}\n",
    "\n",
    "assert_in_delta 4.347826, t32_model.grad([t32_t1], {\"bias\" => 0.77})[\"bias\"], 1e-3\n",
    "assert_in_delta -1.29870, t32_model.grad([t32_t2], {\"bias\" => 0.77})[\"bias\"], 1e-3\n",
    "assert_in_delta 3.049124, t32_model.grad([t32_t1, t32_t2], {\"bias\" => 0.77})[\"bias\"], 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.4 (5 Points)\n",
    "\n",
    "Putting the objective function to work, use gradient descent to find the parameter for the binomial distribution. You get to set the learning rate. Return the learning rate you have obtained which works well. You may have to try a few until you get it right.\n",
    "\n",
    "Note that, while capable of returning the same value, this method reads the data many more times than the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9afa2f41321b339e15c344f2cc4070e5",
     "grade": false,
     "grade_id": "cell-ca57b6ffd6468fe0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def q34_learning_rate\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "813f8a3b8fd578724330c8b2bc5908ab",
     "grade": true,
     "grade_id": "cell-03c7000ad0157d31",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t34 = BinomialModel.new\n",
    "t34_w_init = {\"bias\" => rand}\n",
    "t34_data = coin_dataset(10000)\n",
    "\n",
    "t34_learning_rate = q34_learning_rate()\n",
    "\n",
    "t34_total_loss = 0.0\n",
    "t34_iterations = []\n",
    "t34_losses = []\n",
    "t34_w = t34_w_init\n",
    "gradient_descent t34_data, t34_w_init, t34, t34_learning_rate, 0.001, 100 do |w, iter, loss|\n",
    "  puts [iter, w, t34_losses[-1]].join(\"\\t\") if iter % 10 == 1\n",
    "  t34_total_loss += loss\n",
    "  t34_iterations << iter\n",
    "  t34_losses << t34_total_loss / iter.to_f\n",
    "  t34_w = w\n",
    "end\n",
    "\n",
    "\n",
    "assert_true(t34_losses[-1] < 8000)\n",
    "assert_true(t34_losses[-1] > 0)\n",
    "assert_true(t34_iterations[-1] > 30)\n",
    "assert_in_delta 0.77, t34_w[\"bias\"], 5e-2\n",
    "\n",
    "### Plot the cumulative loss per iteration\n",
    "Daru::DataFrame.new({x: t34_iterations, y: t34_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Iteration\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1 (10 Points)\n",
    "\n",
    "The maximum likelihood method above reads the data multiple times and can benefit from prior knowledge in the form of a prior distribution for the parameter, $\\mu$. Using the Beta distribution as the conjugate prior, implement the likelihood function and its gradient. Now we are learning three parameters altogether: $w_{bias} = \\mu$, $\\alpha$, $\\beta$.\n",
    "\n",
    "First, let's compute the closed form estimator for $\\mu$ with a fixed $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7fe610db1d60504b6baacd8c486dcfde",
     "grade": false,
     "grade_id": "cell-97d4bb1bc5d5a0f3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def q41_closed_form_beta_binomial(coin_data, alpha, beta)\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dbb93934dc0576a9fa0bfa44d94563a1",
     "grade": true,
     "grade_id": "cell-e089341132a19f54",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t41_coin_data = coin_dataset(10000)\n",
    "assert_in_delta 0.77, q41_closed_form_beta_binomial(t41_coin_data, 7743, 10000 - 7743), 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2 (10 points)\n",
    "\n",
    "Implement the negative log likelihood function for the beta + binomial values. Checkout this [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Maximum_likelihood) definition of the likelihood function. Remember we are interested in he negative log likelihood.\n",
    "\n",
    "A special function is needed ```GSL::Sf::lnbeta(a, b)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ae3b18deff351186867bca1e042c1992",
     "grade": false,
     "grade_id": "cell-0c423e99f1c034e0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class BetaBinomialModel\n",
    "  def func dataset, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  def adjust w\n",
    "    w[\"bias\"] = [[0.001, w[\"bias\"]].max, 0.999].min\n",
    "    w[\"beta\"] = [0.0001, w[\"beta\"]].max\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "18a9751dfa872fc62f990d54afd87b64",
     "grade": true,
     "grade_id": "cell-c3da75938a85ae1b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t42 = BetaBinomialModel.new\n",
    "srand 777 #seed random number generator\n",
    "t42_data = coin_dataset(1000)[\"data\"]\n",
    "t42_w = Hash.new {|h,k| h[k] = 0.1}\n",
    "t42_w[\"alpha\"] = 7.0\n",
    "t42_w[\"beta\"] = 3.0\n",
    "\n",
    "assert_in_delta 10373.126026759332, t42.func(t42_data, t42_w), 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3 (10 points)\n",
    "\n",
    "Implement the negative log likelihood gradient for all the parameters. Checkout this [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Maximum_likelihood) definition of the likelihood function. Remember we are interested in he negative log likelihood.\n",
    "\n",
    "A special function is needed ```GSL::Sf::psi(a + b)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "17bd13df38d687cdb957a67a4a762db0",
     "grade": false,
     "grade_id": "cell-9d4f8012ea73cfa8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class BetaBinomialModel\n",
    "  def grad dataset, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d0cedb739f3c1aeb50cc2c127d2be97",
     "grade": true,
     "grade_id": "cell-2752de436c18d1fd",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t43 = BetaBinomialModel.new\n",
    "srand 777 #seed random number generator\n",
    "t43_data = coin_dataset(1000)[\"data\"]\n",
    "t43_w = Hash.new {|h,k| h[k] = 0.1}\n",
    "t43_w[\"alpha\"] = 7.0\n",
    "t43_w[\"beta\"] = 3.0\n",
    "\n",
    "t43_grad = t43.grad(t43_data, t43_w)\n",
    "\n",
    "assert_in_delta -65622.22222222318, t43_grad[\"bias\"], 1e2\n",
    "assert_in_delta 1923.6168390257724, t43_grad[\"alpha\"], 1e2\n",
    "assert_in_delta -1223.6077383104214, t43_grad[\"beta\"], 1e2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.4 (20 points)\n",
    "\n",
    "Run the gradient descent by selecting the initial weights and learning rate. Try a few values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "706ade57f59434796197e6bd977d6a95",
     "grade": false,
     "grade_id": "cell-963091d78ac84fe4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def q44_weights_and_learning_rate\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return [w, lr]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0956c73571f62590c564edb4883248d0",
     "grade": true,
     "grade_id": "cell-2bf436344900b4d6",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t44 = BetaBinomialModel.new\n",
    "t44_data = coin_dataset(10000)\n",
    "\n",
    "t44_w_init, t44_learning_rate = q44_weights_and_learning_rate()\n",
    "\n",
    "t44_total_loss = 0.0\n",
    "t44_iterations = []\n",
    "t44_losses = []\n",
    "t44_w = t34_w_init\n",
    "gradient_descent t44_data, t44_w_init, t44, t44_learning_rate, 0.001, 100 do |w, iter, loss|\n",
    "  puts [iter, w, t44_losses[-1]].join(\"\\t\") if iter % 10 == 1\n",
    "  t44_total_loss += loss\n",
    "  t44_iterations << iter\n",
    "  t44_losses << t34_total_loss / iter.to_f\n",
    "  t44_w = w\n",
    "end\n",
    "\n",
    "\n",
    "assert_true(t44_losses[-1] < 8000)\n",
    "assert_true(t44_losses[-1] > 0)\n",
    "assert_true(t44_iterations[-1] > 30)\n",
    "assert_in_delta 0.77, t44_w[\"bias\"], 5e-2\n",
    "\n",
    "### Plot the cumulative loss per iteration\n",
    "Daru::DataFrame.new({x: t44_iterations, y: t44_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Iteration\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.5.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
