{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f30a623cd239faa48d016dfdfd44f9d",
     "grade": false,
     "grade_id": "cell-2db1ec9fd61f5d6b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 5: Linear Models (2)\n",
    "\n",
    "\n",
    "In this exercise, we will transform our gradient descent algorithm into stochastic gradent descent. We will then implement linear regression and logistic regression. Finally, we will run these models on a real-world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9ca0d151ba43bb4deb857d7f8c49f505",
     "grade": false,
     "grade_id": "cell-af1d85683fc29192",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "require './assignment_lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 (10 points)\n",
    "\n",
    "Transform your batch gradient descent into a stochastic gradient descent algorithm. Using a class here allows the SGD algorithm to maintain state, such as learning rate and weights. Test your algorithm on the coin dataset and Binomial model you coded in Assignment 3. Plot the likelihood measured for each batch of 100 examples on one pass of the dataset.\n",
    "\n",
    "We will implement mini-batch SGD. Therefore you will not have to alter your Binomial Model. \n",
    "\n",
    "Stochasic gradient descent requires a learning rate that decreases with each iteration. Use the following learning rate:\n",
    "\n",
    "## $\\eta = \\frac{\\eta_{0}}{\\sqrt{t}}$\n",
    "\n",
    "where $\\eta_{0}$ is the initial learning rate and $t$ is the number of mini-batch iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9660cfb6ed8d474caab10b4cd3ecccbb",
     "grade": false,
     "grade_id": "cell-8692d024ef034147",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class StochasticGradientDescent\n",
    "  attr_reader :weights\n",
    "  attr_reader :objective\n",
    "  def initialize obj, w_0, lr = 0.01\n",
    "    @objective = obj\n",
    "    @weights = w_0\n",
    "    @n = 1.0\n",
    "    @lr = lr\n",
    "  end\n",
    "  def update x\n",
    "    # BEGIN YOUR CODE\n",
    "    wTemp = @weights.clone\n",
    "    wTemp.each dp |k, v|\n",
    "      wTemp[k] = \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd7cd791abbea9dfafe3ddfeaf2ea601",
     "grade": true,
     "grade_id": "cell-653e55c495e9df88",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "# Testing on a known objective\n",
    "class ParabolaObjective\n",
    "  def func x, w\n",
    "    0.5 * ((w[\"0\"] - 1) ** 2.0 + (w[\"1\"] - 2) ** 2.0)\n",
    "  end\n",
    "  def grad x, w\n",
    "    dw = {\"0\" => (w[\"0\"] - 1), \"1\" => (w[\"1\"] - 2)}\n",
    "  end\n",
    "  def adjust w\n",
    "  end\n",
    "end\n",
    "\n",
    "t1_w = {\"0\" => 0.0, \"1\" => 0.0}\n",
    "t1_obj = ParabolaObjective.new\n",
    "t1_sgd = StochasticGradientDescent.new t1_obj, t1_w, 0.25\n",
    "t1_lik = 1.0\n",
    "1000.times do \n",
    "  t1_sgd.update([])\n",
    "  t1_lik = t1_obj.func([], t1_sgd.weights)\n",
    "end\n",
    "\n",
    "assert_true(t1_lik < 0.1, \"SGD converges with simple objective\")\n",
    "assert_in_delta 1.0, t1_sgd.weights[\"0\"], 0.1, \"Weight 0 expected to be 1.0\"\n",
    "assert_in_delta 2.0, t1_sgd.weights[\"1\"], 0.1, \"Weight 1 expected to be 2.0\"\n",
    "\n",
    "t1_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Question 2.1 (10 points)\n",
    "\n",
    "Implement linear regression as an objective function for use with stochastic gradient descent. First, we will implement the predict function. For a weight vector, $w$ and a single ```Row``` with features, $x$, implement:\n",
    "\n",
    "### $f(w,x) = w^T x$\n",
    "\n",
    "Note that you already did this in [Assignment 4](../assignment-4/assignment-4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0d1bad1b96670e69c22ab3eb42d447cd",
     "grade": false,
     "grade_id": "cell-513d6dcc72afdb50",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModel  \n",
    "  def predict row, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d00bc15f77435d218fb4a379e4de2411",
     "grade": true,
     "grade_id": "cell-b49b45d9c251f254",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t2_lr = LinearRegressionModel.new\n",
    "\n",
    "assert_in_delta 6.0, t2_lr.predict({\"features\" => {\"a\" => 2.0}}, {\"a\" => 3.0}), 1e-6\n",
    "assert_in_delta 6.0, t2_lr.predict({\"features\" => {\"a\" => 2.0}}, {\"a\" => 3.0, \"b\" => 4.0}), 1e-6\n",
    "assert_equal 0.0, t2_lr.predict({\"features\" => {}}, {})\n",
    "assert_equal 0.0, t2_lr.predict({\"features\" => {\"a\" => 1.0}}, {\"a\" => 0.0, \"b\" => 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Question 2.2 (10 points)\n",
    "\n",
    "Continuing the implementation, implement the $L_2$ loss, which applies to a mini-batch of $n$ points. Use the ```predict``` function you implemented earlier.\n",
    "\n",
    "### $L(w,X) = \\frac{1}{n} \\sum_{i} \\frac{1}{2} \\left(f(w,x_i) - y_i\\right) ^ 2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a3084f061b575feda9fe737e5bbd4611",
     "grade": false,
     "grade_id": "cell-ef24b617c1a0ecd5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModel\n",
    "  def func data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  ## Adjusts the parameter to be within the allowable range\n",
    "  def adjust w\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4e0b66491efad74390ec2fb3575a7556",
     "grade": true,
     "grade_id": "cell-930a1ec55478e6d9",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t22_data = coin_dataset(1000)\n",
    "\n",
    "t22_model = LinearRegressionModel.new\n",
    "t22_w = Hash.new\n",
    "t22_w[\"bias\"] = 0.1\n",
    "\n",
    "t22_f = t22_model.func t22_data[\"data\"], t22_w\n",
    "assert_in_delta 0.300, t22_f, 0.050, \"Expected loss within [250, 350]\"\n",
    "\n",
    "t22_w[\"bias\"] = 0.77\n",
    "t22_f = t22_model.func t22_data[\"data\"], t22_w\n",
    "assert_in_delta 0.090, t22_f, 0.050, \"Expected loss for a closer guess to be within [40, 140]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Question 2.3 (10 points)\n",
    "\n",
    "Continuing the implementation, now implement the gradient function. This returns a gradient value for the mini-batch of $n$ points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9755d5d5e1bfe507dd8972b5636d7b42",
     "grade": false,
     "grade_id": "cell-d96f7fe6f72bf82c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModel\n",
    "  def grad data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ced679c62665c25f724cd2b6401d44f",
     "grade": true,
     "grade_id": "cell-6581a7319efd449e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "\n",
    "t23_data = coin_dataset(1000)\n",
    "t23_model = LinearRegressionModel.new\n",
    "t23_w = Hash.new\n",
    "t23_w[\"bias\"] = 0.1\n",
    "\n",
    "t23_g = t23_model.grad t23_data[\"data\"], t23_w\n",
    "assert_in_delta -0.69, t23_g[\"bias\"], 0.2, \"Expected loss within [-0.49, -0.89]\"\n",
    "\n",
    "t23_w[\"bias\"] = 0.77\n",
    "t23_g = t23_model.grad t23_data[\"data\"], t23_w\n",
    "assert_in_delta 0.0, t23_g[\"bias\"], 0.1, \"Expected loss for a better guess to be within [-0.1, 0.1]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4 (10 points)\n",
    "\n",
    "Putting the previous steps together, use your ```StochasticGradientDecent``` to run linear regression for 10 passes (epochs) over the Coin Dataset, each pass with a mini-batch of size 20. Tune the learning rate, ```lr```, so that the model converges well. Assume that ```obj``` is an instance of ```LinearRegressionModel```, and ```w``` is an initial weight vector.\n",
    "\n",
    "Track the number of batches in the ```iters``` array and the loss in the ```losses``` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bc9391a560e2495790954502116709ca",
     "grade": false,
     "grade_id": "cell-c5159a9b8d15fac9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_coin_sgd(obj, w, dataset)\n",
    "  i = 0\n",
    "  iters = []\n",
    "  losses = []\n",
    "  \n",
    "  #Define sgd = StochasticGradientDescent.new obj, w, lr\n",
    "  # You set the learning rate, lr\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return [sgd, iters, losses]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2897553eb061eeeebdbfeec1bf56c681",
     "grade": true,
     "grade_id": "cell-ad954908a6c9a9cb",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t24_data = coin_dataset(1000)\n",
    "t24_model = LinearRegressionModel.new\n",
    "t24_w = Hash.new\n",
    "t24_w[\"bias\"] = 0.1\n",
    "\n",
    "t24_trainer, t24_iter, t24_losses = train_coin_sgd t24_model, t24_w, t24_data\n",
    "\n",
    "assert_true t24_w.has_key?(\"bias\")\n",
    "assert_in_delta 0.77, t24_w[\"bias\"], 0.1, \"Expected weight for 'bias'  [0.67, 0.87]\"\n",
    "t24_cum_loss = 0.0\n",
    "t24_losses.each_index {|i| t24_cum_loss += t24_losses[i]; t24_losses[i] = t24_cum_loss / (t24_iter[i] + 1)}\n",
    "Daru::DataFrame.new({x: t24_iter, y: t24_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Batches\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2418aa0f48abdbe6ff014a29532ccb6",
     "grade": false,
     "grade_id": "cell-b41019cfe2cced3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 3.1 (10 points)\n",
    "\n",
    "Implement Logistic Regression, following much the same process as with linear regression. The prediction function returns a value:\n",
    "\n",
    "### $f(x,w) = \\frac {1}{1 + \\exp \\left( -w^T x \\right) } $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "778b87558628d1fdb62e6ddb4d0bd42d",
     "grade": false,
     "grade_id": "cell-33bf6c407e0d7157",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionModel\n",
    "  def predict row, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def adjust w\n",
    "    w\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a4047451ff4933f13f0817b7009ea650",
     "grade": true,
     "grade_id": "cell-88560ef08ee29cdf",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t31_model = LogisticRegressionModel.new\n",
    "def t31_f(a: 0.0, b: 0.0)\n",
    "  row = {\"features\" => {\"a\" => a, \"b\" => b}}\n",
    "end\n",
    "def t31_w(a: 0.0, b: 0.0)\n",
    "  w = {\"a\" => a, \"b\" => b}\n",
    "end\n",
    "assert_in_delta 0.5, t31_model.predict(t31_f(), t31_w()), 1e-6\n",
    "assert_in_delta 0.2689, t31_model.predict(t31_f(a:1), t31_w(a:-1)), 1e-3\n",
    "assert_in_delta 1.0, t31_model.predict(t31_f(a:1, b:1000), t31_w(a:-1, b: 0.1)), 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2 (10 points)\n",
    "\n",
    "Implement log loss assuming that the y label is defined as: $y \\in \\left\\{-1, 1\\right\\}$. Remember that the mini-batch loss is an expectation of the $n$ examples in the mini batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1caeabc05bb45b962b6f48b4f7f5df23",
     "grade": false,
     "grade_id": "cell-15878f1a66886147",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionModel\n",
    "  def func data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "db1f969ddf5fd5244aae153ae2863612",
     "grade": true,
     "grade_id": "cell-9dc5beeddbd5c30b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t32_data = coin_dataset(1000)\n",
    "t32_model = LogisticRegressionModel.new\n",
    "t32_w = Hash.new {|h,k| h[k] = 0.1}\n",
    "assert_in_delta 0.66, t32_model.func(t32_data[\"data\"], t32_w), 0.2, \"Expected LR.func in [460, 860]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3 (10 points)\n",
    "\n",
    "Calculate the gradient of the mini-batch log loss for each parameter $w$. This time, assume that $y \\in \\left\\{0, 1\\right\\}$. Hint: This assumption should simplify the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b55f5b3f1de2689550894e67480e8fc3",
     "grade": false,
     "grade_id": "cell-35515a097456be8f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionModel\n",
    "  def grad data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc43d49ef3013a4d2a8b6943fc30f895",
     "grade": true,
     "grade_id": "cell-a546f94a5ee061c0",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t32_data = coin_dataset(1000)\n",
    "t32_model = LogisticRegressionModel.new\n",
    "t32_w = Hash.new {|h,k| h[k] = 0.1}\n",
    "assert_in_delta 0.66, t32_model.func(t32_data[\"data\"], t32_w), 0.2, \"Expected LR.func in [460, 860]\"\n",
    "t32_g = t32_model.grad t32_data[\"data\"], t32_w\n",
    "assert_in_delta -0.26, t32_g[\"bias\"], 0.1, \"Expected LR.grad in [-0.36, -0.16]\"\n",
    "\n",
    "t32_w = Hash.new {|h,k| h[k] = 0.778}\n",
    "t32_g = t32_model.grad t32_data[\"data\"], t32_w\n",
    "assert_in_delta -0.1, t32_g[\"bias\"], 0.1, \"Expected LR.grad for a closer value to be in [-0.2, -0.2]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1 (6 points)\n",
    "\n",
    "Let's train our new models on a familiar dataset, spambase. Let's run gradient descent for a few steps on this dataset. Observe that the learned weights after just gradient 2 steps are very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example ###\n",
    "#Preview 2 lines from the Spambase dataset\n",
    "spambase = read_sparse_data_from_csv \"spambase\"\n",
    "spambase[\"data\"].each {|r| r[\"features\"][\"bias\"] = 1.0}\n",
    "puts spambase[\"data\"][0,2]\n",
    "\n",
    "q41_model = LinearRegressionModel.new\n",
    "q41_w = Hash.new {|h,k| h[k] = 0.0}\n",
    "q41_w[\"bias\"] = 1\n",
    "q41_sgd = StochasticGradientDescent.new q41_model, q41_w, 0.1\n",
    "2.times do\n",
    "  q41_batch = spambase[\"data\"].sample(10)\n",
    "  q41_sgd.update q41_batch\n",
    "end\n",
    "puts q41_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1 (Continued) \n",
    "We can correct this by _normalizing_ the data. A popular normalization is the z-score. For each feature, except bias, and considering only the non-zero values create a new zspambase dataset, ```zspambase```. The dataset ```zspambase``` is identical to spambase except that its features have been normalized as follows:\n",
    "\n",
    "### $x_z = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "where $\\mu$ is the mean of the $x$ value and $\\sigma$ is the standard deviation. Note that you have already seen an implementation of ```mean``` and ```stdev```, so find it and add it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "be552b33440500fbf331278317542948",
     "grade": false,
     "grade_id": "cell-ad476d8f1e9bb351",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Add mean and stdev here\n",
    "\n",
    "# BEGIN YOUR CODE\n",
    "raise NotImplementedError.new()\n",
    "#END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bafc508212cf426a2ec9755991c87569",
     "grade": false,
     "grade_id": "cell-481076f8bac87071",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_zspambase spambase\n",
    "  zspambase = spambase.clone\n",
    "  zspambase[\"data\"] = spambase[\"data\"].collect do |r|\n",
    "    u = r.clone\n",
    "    u[\"features\"] = r[\"features\"].clone\n",
    "    u\n",
    "  end\n",
    "\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return zspambase\n",
    "end\n",
    "\n",
    "zspambase = create_zspambase spambase\n",
    "zspambase[\"data\"].first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c4d4f5f9d7a599ca30d4aaa83198e29c",
     "grade": true,
     "grade_id": "cell-f9f979fc45ab2a30",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t41_zs = create_zspambase spambase\n",
    "\n",
    "assert_in_delta 0.27, spambase[\"data\"].first[\"features\"][\"word_freq_our\"], 1e-5\n",
    "assert_in_delta -0.628106690674003, zspambase[\"data\"].first[\"features\"][\"word_freq_our\"], 1e-5\n",
    "\n",
    "assert_in_delta 607.0, spambase[\"data\"].first[\"features\"][\"capital_run_length_total\"], 1e-5\n",
    "assert_in_delta 0.53386, zspambase[\"data\"].first[\"features\"][\"capital_run_length_total\"], 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2 (7 points)\n",
    "\n",
    "Train Linear Regression for the ```zspambase``` dataset. Tune the learning rate as needed to train in one epoch. Hint: Learning rate may need to be very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3c4ddd54c16eb6b7d485cf6c77497441",
     "grade": false,
     "grade_id": "cell-534aff92ed1aadf8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_zspambase_sgd(obj, w, dataset)\n",
    "  i = 0\n",
    "  iters = []\n",
    "  losses = []\n",
    "  \n",
    "  #Define sgd = StochasticGradientDescent.new obj, w, lr\n",
    "  # You set the learning rate, lr\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return [sgd, iters, losses]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7881aca4101647e3c85ee5ccb52c67b6",
     "grade": true,
     "grade_id": "cell-1b5de61217794c41",
     "locked": true,
     "points": 7,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t25_model = LinearRegressionModel.new\n",
    "t25_w = Hash.new {|h,k| h[k] = 0.0}\n",
    "t25_w[\"bias\"] = 1\n",
    "\n",
    "t25_trainer, t25_iter, t25_losses = train_zspambase_sgd t25_model, t25_w, zspambase\n",
    "puts t25_w\n",
    "\n",
    "t25_cum_loss = 0.0\n",
    "t25_losses.each_index {|i| t25_cum_loss += t25_losses[i]; t25_losses[i] = t25_cum_loss / (t25_iter[i] + 1)}\n",
    "assert_true (t25_losses.last < 0.15), \"Expected last loss value less than target\"\n",
    "Daru::DataFrame.new({x: t25_iter, y: t25_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Batches\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3 (7 points)\n",
    "\n",
    "Run logistic regression on the ```zspambase``` dataset, tuning the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "41ad8150351136ed6e4631337bb20d57",
     "grade": false,
     "grade_id": "cell-7fd32c360f47ff13",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_zspambase_logistic_sgd(obj, w, dataset)\n",
    "  i = 0\n",
    "  iters = []\n",
    "  losses = []\n",
    "  \n",
    "  #Define sgd = StochasticGradientDescent.new obj, w, lr\n",
    "  # You set the learning rate, lr\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return [sgd, iters, losses]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0efa9ebe30c126d1b59a9bbc3341688",
     "grade": true,
     "grade_id": "cell-a2ee6be6489349a5",
     "locked": true,
     "points": 7,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t43_model = LogisticRegressionModel.new\n",
    "t43_w = Hash.new {|h,k| h[k] = 0.0}\n",
    "t43_w[\"bias\"] = 1\n",
    "\n",
    "t43_trainer, t43_iter, t43_losses = train_zspambase_logistic_sgd t43_model, t43_w, zspambase\n",
    "t43_cum_loss = 0.0\n",
    "t43_losses.each_index {|i| t43_cum_loss += t43_losses[i]; t43_losses[i] = t43_cum_loss / (t43_iter[i] + 1)}\n",
    "puts t43_w\n",
    "\n",
    "assert_true(t43_losses.last < 0.6, \"Expected last loss value < 0.6\")\n",
    "Daru::DataFrame.new({x: t43_iter, y: t43_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Batches\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.5.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
