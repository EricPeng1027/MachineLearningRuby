{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7c336275b386d6439ea67e77e96d9045",
     "grade": false,
     "grade_id": "cell-d21a86d292a3d7ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Final Project Report\n",
    "\n",
    "Write your final project report in the cells below. All written material must be completed prior to your presentation slot. Including figures, appendices, etc. a printed version of this document should be no more than 10 pages and no less than 6 pages. \n",
    "\n",
    "The following cell is used for overall feedback and deductions for length, content, and style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ce7ea5533b1effff2e13157180854ce3",
     "grade": true,
     "grade_id": "cell-343bed4feca7930f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b771f443165dfa2917170d3a0b9191d8",
     "grade": false,
     "grade_id": "cell-d9bb43612fd5686f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Author e-mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "91298380b4a3751e9cd35f8c35a47170",
     "grade": true,
     "grade_id": "cell-f96ae95e5a641ea0",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "peng.yuc@husky.neu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "_(Why would people want to study this dataset and what is the primary task. Find out what the \"target\" variable means and why the customer is interested in running a competition on this dataset.)_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1b7d89f8e38774cd0b2d59d09f04a82b",
     "grade": true,
     "grade_id": "cell-c9bb94ed861b91bf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n",
    "\n",
    "Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n",
    "\n",
    "While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propsed method\n",
    "_(Present an 1-paragraph description of your method and why you believe it is better that the other things you have tried)_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ad319c78971c6e4155af9a1e0aa2cf64",
     "grade": true,
     "grade_id": "cell-6085b28e8af4d158",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "I didn’t have a clue when starting seeking for features. There are hundreds of variables, how would I choose features so that they work for our model. I calculate their correlation to the target to analyze their feasibilities, and plot their distribution to see their impact on the target. Finally I extract some useful features. When it comes to model selection, what I’ve learned is restricted to some basic models. Intuitively, I start with Linear Regression. Of course, there can’t be a good result with that. Then I switch to Logistic Regression, the AUC gets a lot better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work\n",
    "_(Find (5-6) examples of people who have worked on similar dataset from the literature. Note: Literature == Published paper in a conference (not stack overflow). Briefly describe in 1-2 sentences the kinds of features, algorithms, or other methods they applied. Also explain why you believe your method is better. Provide a link to the cited paper with a number for each citation indicating where one can download the paper from a popular site [[1]](https://arxiv.org).)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ede8e26b7174b9d514f4cf789af54bde",
     "grade": true,
     "grade_id": "cell-299d7142a9907f1d",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Feature Selection for Classification[1]\n",
    "\n",
    "This survey is a comprehensive overview of many existing methods to select feature from the 1970's to the present.\n",
    "\n",
    "\n",
    "Musculoskeletal Model of the Upper Limb Based on the Visible Human Male Dataset[2]\n",
    "\n",
    "A mathematical model of Ihe human upper limb was developed based on high-resolution medical images of the muscles and bones obtained from the Visible Human Male (HM) project.\n",
    "\n",
    "\n",
    "Longitudinal data analysis using generalized linear models[3]\n",
    "\n",
    "This paper proposes an extension of generalized linear models to the analysis of longitudinal data.\n",
    "\n",
    "\n",
    "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy[4]\n",
    "\n",
    "This article teaches us how to select good features according to the maximal statistical dependency criterion based on mutual information.\n",
    "\n",
    "\n",
    "Classifier Chains for Multi-label Classification[5]\n",
    "\n",
    "This paper presented a novel chaining method for multi-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related implementations\n",
    "_(Find (2-3) examples of what people in Kaggle have done on this particular dataset [[2]](https://www.kaggle.com). Reference the URL of their kernel, post, etc. Describe in 1-2 sentences what they have done and why you think your method is better.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5b53e3ee2a6b1cb541dd35a70f821025",
     "grade": true,
     "grade_id": "cell-a30ca527d1596437",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "[HomeCreditRisk: Extensive EDA + Baseline [0.772]](https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772) Shivam Bansal\n",
    "\n",
    "He explored all the features from these seven tables and gave us a plot to show the most important features which was very helpful. Then he implemented a baseline model to make the prediction.\n",
    "\n",
    "[Start Here: A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/notebook) Will Koehrsen\n",
    "\n",
    "He gave us a brief introduction of the dataset and the basic knowledge of machine learning. After preprocessing the data like dealing with missing value, using label encoding and one-hot encoding to encode categorical variables, he figured out some interesting features which would help to make predictions by using correlation and pairs plot. Then, he constructed new features out of the existing data to see if these new features could help the model. Finally he implemented a baseline model and a second more complicated model.\n",
    "\n",
    "In this post, he only analyzed the data in application_train dataset, however, there are six more tables which weren't explored. I used the data from different tables and got a better AUC score than his model.\n",
    "\n",
    "[Kaggle Champion](https://www.kaggle.com/c/home-credit-default-risk/discussion/64821)\n",
    "\n",
    "From my prior experience with credit underwriting, I’ve come to appreciate that this is one of the most complex problems for applying machine learning to. Data in this domain tends to be very heterogeneous, collected over different time frames, and coming from many different sources that may change and alter in midst of the data collection process. Coming up with a proper target variable is also a very tricky process, that requires deep domain knowledge and refined business analysis skills. I want, again, to commend Home Credit and Kaggle for coming up with such a great dataset, that was leak-free and very amenable to Machine Learning techniques.\n",
    "\n",
    "Based on what is known about Credit Underwriting, and in general these kinds of Machine Learning problems, it has been clear all along that there would be two things crucial for building a good model for this competition: 1. Good set of smart features. 2. Diverse set of base algorithms. We were able to have four main sources of feature diversity, and a few minor additional ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "_(Data Analysis: Describe the data analysis you have completed, include 1-2 plots of the most useful features or learnings you have obtained from the dataset. Do not include the code, but do include formulas to anything you have calculated such as different feature combinations, feature selection, or analysis methods. You must use at least one clustering algorithm we have seen in class for an analysis of the data. Provide a link to the specific notebook cell in previous notebooks as a reference.)_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dadf3d2d4ff41cb2e933d964f940c8b4",
     "grade": true,
     "grade_id": "cell-f6715c211519644e",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "application_{train|test}.csv\n",
    "\n",
    "This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n",
    "Static data for all applications. One row represents one loan in our data sample.\n",
    "\n",
    "bureau.csv\n",
    "\n",
    "All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\n",
    "\n",
    "For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n",
    "\n",
    "bureau_balance.csv\n",
    "\n",
    "Monthly balances of previous credits in Credit Bureau.\n",
    "This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\n",
    "\n",
    "POS_CASH_balance.csv\n",
    "\n",
    "Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n",
    "This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\n",
    "\n",
    "credit_card_balance.csv\n",
    "\n",
    "Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n",
    "This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\n",
    "\n",
    "previous_application.csv\n",
    "\n",
    "All previous applications for Home Credit loans of clients who have loans in our sample.\n",
    "There is one row for each previous application related to loans in our data sample.\n",
    "\n",
    "installments_payments.csv\n",
    "\n",
    "Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n",
    "There is a) one row for every payment that was made plus b) one row each for missed payment.\n",
    "One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.\n",
    "\n",
    "<img src=\"Correlation_Plot_Application_Train.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "<img src=\"EXT_SOURCE_3_PLOT.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "<img src=\"DAYS_BIRTH_PLOT.png\" alt=\"drawing\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Methods\n",
    "_(Describe the ML algorithms you used for questions [3.1](part-3.ipynb#Question-3.1), [4.1](part-3.ipynb#Question-4.1), and all others. Focus on the formulas, any feature extractions, parameter tuning, etc. Explain how the algorithm works. E.g., if you used a decision, don't say \"I used a decision tree\", explain briefly how a decision tree works and why it was ideally suited for the dataset you chose.)_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cdb42ea3806602e6aa3782bb2af3f2aa",
     "grade": true,
     "grade_id": "cell-616d5470b36b8952",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Linear Regression\n",
    "\n",
    "<img src=\"capture.JPG\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "When applied in this project\n",
    "\n",
    "X represents the features\n",
    "\n",
    "Y represent the prediction whether the client will pay the loan, when y is 0, the client will pay it, otherwise won’t\n",
    "\n",
    "w0 & wk  are intercept and weights respectively\n",
    "\n",
    "Features selected\n",
    "\n",
    "FLAG_DOCUMENT_3\n",
    "\n",
    "Operations about missing values\n",
    "\n",
    "Set the missing values to -1\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "<img src=\"capture1.JPG\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "When applied in this project\n",
    "\n",
    "X represents the features\n",
    "\n",
    "Y represents the prediction whether the client will pay the loan, when y is 0, the client will pay it, otherwise won’t\n",
    "\n",
    "W represents the weight vector\n",
    "\n",
    "Features selected\n",
    "\n",
    "EXT_SOURCE_3\n",
    "\n",
    "Operations about missing values\n",
    "\n",
    "Set the missing values to -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "_(Provide some insights as to why you think that the proposed algorithms and features are good for this dataset. Explain whether you believe these are general properties that might be helpful for similar datasets--what makes them similar and why. What about this dataset made your solution successful. Could we use this for other datasets, if so, what types and why?)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9ae0fa2d0474dd5cc8c4555d77506c12",
     "grade": true,
     "grade_id": "cell-a5fad0534cafdb6f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "I did clustering about all the features then I calculate the correlation of each featureto the target. The most positive/negative correlation features have the most potential to be the good one. Plus we could not only use those single features but combinations of them. If the AUC proves to be a good one, we consider this feature or combination to be the one adapted to our final feature choice and apply them to do the prediction for our dataset.\n",
    "\n",
    "For other similar dataset, we can also use our model to make prediction if the target column is binary variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "_(Did you use all the data, cross-validation, training / test split, etc? Give enough details on how you setup the experiment so that your colleague can read this section and write their own algorithm to produce the same setup. Provide a link to the cells in the notebooks that contain the experimental setup.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7124b6b05c26327c785ea4baa791ee03",
     "grade": true,
     "grade_id": "cell-8e284da64965d4fd",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "I used the data in application_train and bureau tables and training/test split.\n",
    "\n",
    "I did k-means clustering of the dataset and calculate the correlation of each feature to the target to get the feature that is most suitable for this project.\n",
    "\n",
    "Then it comes to model selection, I start with the naive one - Linear Regression and deal with those missing values by setting them to -1. The feature I select is FLAG_DOCUMENT_3 and the AUC I got is a little over 0.5.\n",
    "\n",
    "However, Linear Regression model is too simple which has lots of restrictions when advancing, thus I switch to Logistic Regression model and utilize more complicated features, the performance gets a lot better. The AUC is raised to over 0.68\n",
    "\n",
    "For improvement, I try to make some change, I normalize the data and set those missing values to the mean value instead of setting them to -1, the final result gets even better which is over 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "_(Write a table containing the results of your experiments, which were calculated in the notebooks. Include all algorithms included in questions [3.1](part-3.ipynb#Question-3.1) and above in Part 3 of the notebooks. Provide some interpretation of these results. Do you think you could have done better? If so, why did you not pursue those ideas? Add any pictures you think approprate here.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "47b0ae14537531217347e46db447bbba",
     "grade": true,
     "grade_id": "cell-384dadfd814ac391",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "| Model               | Features                                                                                                                                                                                                                 |                       Operations                      | AUC    |   |\n",
    "|---------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------:|--------|---|\n",
    "| Linear Regression   | FLAG_DOCUMENT_3                                                                                                                                                                                                          |                set missing values to -1               | 0.5354 |   |\n",
    "| Logistic Regression | EXT_SOURCE_3                                                                                                                                                                                                             |                set missing values to -1               | 0.6125 |   |\n",
    "| Logistic Regression | ext_source_1,ext_source_2, ext_source_3, (AMT_ANNUITY/AMT_CREDIT), DAYS_BIRTH, REGION_RATING_CLIENT, DAYS_LAST_PHONE_CHANGE, FLAG_DOCUMENT_3, REG_REGION_NOT_LIVE_REGION, REG_CITY_NOT_WORK_CITY, REG_CITY_NOT_LIVE_CITY | set missing values to mean vaule and do normalization | 0.7145 |   |\n",
    "\n",
    "There are hundreds of variables,thus we can not only use those single features but create more features of combination. Plus due time and knowledge restriction, I only use linear regression model and logistic regression model, which are the two most simple one. More complicated model could be utilized to achieve better effect. And there're lots of parts need polishment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "_(Summarize your findings. If someone wanted to use your solution, which would you recommend? What could you do if you had more data, etc. What should a company seeking to run this at high scale choose if they were to use your method.)_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bfdbf631dd7ab4a6b983b5f57b05e7d8",
     "grade": true,
     "grade_id": "cell-fac0328f843eb6f5",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "As a Kaggle competition project, it has witnessed numerous fantastic models and learning algorithms. Now it serves as our final project, I can only use what I’ve learned to give it a try.\n",
    "\n",
    "I didn’t have a clue when starting seeking for features. There are hundreds of variables, how would I choose features so that they work for our model. I calculate their correlation to the target to analyze their feasibilities, and plot their distribution to see their impact on the target. Finally I extract some useful features.\n",
    "\n",
    "When it comes to model selection, what I’ve learned is restricted to some basic models. Intuitively, I start with Linear Regression. Of course, there can’t be a good result with that. Then I switch to Logistic Regression, the AUC gets a lot better\n",
    "\n",
    "Finally, I try to improve my project by normalizing data, adjusting missing values and constructing more useful features. The results get better and better.\n",
    "\n",
    "Although this is just a course project, it gives me lots of knowledge about how to complete a whole machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "_(Add a numbered list of the referenced articles, notebooks, etc. that you cited in the above notebooks. Pay attention that the numbers you used correspond to the list below.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c2f84f22b9beec3a8bdc0996e7f0c5c3",
     "grade": true,
     "grade_id": "cell-b608d1f5b8e2570a",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. [Musculoskeletal Model of the Upper Limb Based on the Visible Human Male Dataset](https://www.tandfonline.com/doi/abs/10.1080/10255840008908000)\n",
    "\n",
    "2. [Longitudinal data analysis using generalized linear models](https://academic.oup.com/biomet/article/73/1/13/246001)\n",
    "\n",
    "3. [Feature Selection for Classification](https://content.iospress.com/articles/intelligent-data-analysis/ida1-3-02)\n",
    "\n",
    "4. [Classifier Chains for Multi-label Classification](https://link.springer.com/chapter/10.1007/978-3-642-04174-7_17)\n",
    "\n",
    "5. [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)\n",
    "\n",
    "6. [HomeCreditRisk: Extensive EDA + Baseline [0.772]](https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772)\n",
    "\n",
    "7. [Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy](https://ieeexplore.ieee.org/abstract/document/1453511)\n",
    "\n",
    "8. [Start Here: A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Appendix\n",
    "\n",
    "Add links to your part 1, part 2, and part 3 notebooks (using absolute links). \n",
    "Add anything else you want to here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8f2acbdc305473c633729c1074c52caa",
     "grade": true,
     "grade_id": "cell-a941b5ed7957cf69",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "[part1](http://ec2-54-172-57-139.compute-1.amazonaws.com:31757/user/ericpeng1027/notebooks/final-project-1/part-1.ipynb)\n",
    "\n",
    "[part2](http://ec2-54-172-57-139.compute-1.amazonaws.com:31757/user/ericpeng1027/notebooks/final-project-2/part-2.ipynb)\n",
    "\n",
    "[part3](http://ec2-54-172-57-139.compute-1.amazonaws.com:31757/user/ericpeng1027/notebooks/final-project/part-3.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.5.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
